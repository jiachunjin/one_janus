{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "610dfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6770ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjc/miniconda3/envs/dllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjc/miniconda3/envs/dllm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:604: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-25 09:17:40,403] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjc/miniconda3/envs/dllm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/jjc/miniconda3/envs/dllm/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0c63b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/data1/ckpts/deepseek-ai_/Janus-Pro-1B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = MultiModalityCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1277e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL.Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"/data1/ckpts/deepseek-ai_/Janus-Pro-1B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair\",\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"\",\n",
    ")\n",
    "prompt = sft_format + vl_chat_processor.image_start_tag\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    temperature: float = 1,\n",
    "    parallel_size: int = 16,\n",
    "    cfg_weight: float = 5,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "):\n",
    "    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()\n",
    "    for i in range(parallel_size*2):\n",
    "        tokens[i, :] = input_ids\n",
    "        if i % 2 != 0:\n",
    "            tokens[i, 1:-1] = vl_chat_processor.pad_id\n",
    "\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n",
    "\n",
    "    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "    for i in range(image_token_num_per_image):\n",
    "        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n",
    "        logit_cond = logits[0::2, :]\n",
    "        logit_uncond = logits[1::2, :]\n",
    "        \n",
    "        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "\n",
    "    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "    visual_img[:, :, :] = dec\n",
    "\n",
    "    os.makedirs('generated_samples', exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join('generated_samples', \"img_{}.jpg\".format(i))\n",
    "        PIL.Image.fromarray(visual_img[i]).save(save_path)\n",
    "\n",
    "\n",
    "generate(\n",
    "    vl_gpt,\n",
    "    vl_chat_processor,\n",
    "    prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcdc50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjc/miniconda3/envs/dllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjc/miniconda3/envs/dllm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:604: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-25 11:17:54,175] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjc/miniconda3/envs/dllm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/jjc/miniconda3/envs/dllm/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from util.dataloader import get_dataloader\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "\n",
    "config = OmegaConf.load(\"config/janus_gen.yaml\")\n",
    "dataloader = get_dataloader(config.data)\n",
    "\n",
    "janus = MultiModalityCausalLM.from_pretrained(\"/data1/ckpts/deepseek-ai_/Janus-Pro-1B\", trust_remote_code=True)\n",
    "vl_chat_processor = VLChatProcessor.from_pretrained(\"/data1/ckpts/deepseek-ai_/Janus-Pro-1B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5dd1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150])\n",
      "torch.Size([150])\n",
      "text_embedding torch.Size([1, 151, 2048])\n",
      "text_embedding torch.Size([1, 151, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dtype = torch.float32\n",
    "device = \"cpu\"\n",
    "for batch in dataloader:\n",
    "    janus.train()\n",
    "    text = batch[\"texts\"]\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"].to(dtype)\n",
    "    img_embeddings = janus.aligner(janus.vision_model(pixel_values))\n",
    "    joint_embeddings = []\n",
    "    # img_masks = []\n",
    "    B = pixel_values.shape[0]\n",
    "\n",
    "    for i, input_ids in enumerate(text):\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([100003], device=device)])\n",
    "        # if input_ids.shape[0] > 150:\n",
    "        #     continue\n",
    "        text_embedding = janus.language_model.get_input_embeddings()(input_ids).unsqueeze(0)                    \n",
    "        img_embedding = img_embeddings[i].unsqueeze(0)\n",
    "        joint_embedding = torch.cat((text_embedding, img_embedding), dim=1)\n",
    "        # num_padding = 150 + 576 - joint_embedding.shape[1]\n",
    "        # joint_embedding = torch.cat((joint_embedding, torch.zeros((1, num_padding, 2048), device=device)), dim=1)\n",
    "        \n",
    "        # img_mask = torch.zeros(1, joint_embedding.shape[1], dtype=torch.bool, device=device)\n",
    "        print(\"text_embedding\", text_embedding.shape)\n",
    "        # img_mask[:, text_embedding.shape[1]:text_embedding.shape[1]+576] = 1\n",
    "        # img_masks.append(img_mask)\n",
    "        joint_embeddings.append(joint_embedding)\n",
    "    joint_embeddings = torch.cat(joint_embeddings, dim=0)\n",
    "    # img_masks = torch.cat(img_masks, dim=0)\n",
    "    break\n",
    "\n",
    "    # print(joint_embeddings.dtype)\n",
    "    output = janus.language_model(\n",
    "        inputs_embeds=joint_embeddings.to(dtype),\n",
    "        attention_mask=None,\n",
    "        output_hidden_states=True,\n",
    "    ).hidden_states[-1]\n",
    "\n",
    "    # Create a copy tensor with the same shape as output but filled with zeros\n",
    "    # masked_output = torch.zeros_like(output)\n",
    "    # # Only fill values where padding_masks is True\n",
    "    # masked_output[padding_masks] = output[padding_masks]\n",
    "    print(output.shape, output[output].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ad1bb102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100000, 100003])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_chat_processor.tokenizer([\"<begin_of_image>\"], return_tensors=\"pt\").input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e99cfb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<｜▁pad▁｜>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91b0b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [100000, 100002], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl_chat_processor.tokenizer(vl_chat_processor.pad_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "386ac7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 576, 2048])\n",
      "<｜begin▁of▁sentence｜>The image depicts a person in military attire, including a helmet and tactical gear, running through a grassy field dotted with yellow flowers. The individual appears to be in motion, possibly during a training exercise or a tactical operation. The background shows a mix of greenery and a dirt path, suggesting an open outdoor setting. The person's posture and the way they are holding their hands up might indicate a gesture of surrender or signaling. The overall scene conveys a sense of action and movement in a natural environment.<begin_of_image>\n",
      "torch.Size([1, 726, 2048])\n",
      "torch.Size([678, 2048])\n",
      "<｜begin▁of▁sentence｜>The image captures a picturesque coastal scene with a clear blue sky and scattered clouds. In the foreground, a calm body of water reflects the serene atmosphere. A dock extends into the water, with a flagpole and a small structure at its end. Beyond the dock, a marina is visible, filled with various boats and yachts moored along the pier. The midground features a charming town with white buildings adorned with red roofs, and a prominent bell tower rises above the skyline. In the background, a range of rugged mountains provides a dramatic backdrop, adding depth and grandeur to the landscape. The overall composition exudes tranquility and natural beauty, typical of a Mediterranean coastal town.<begin_of_image>\n",
      "torch.Size([1, 726, 2048])\n",
      "torch.Size([714, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Tokenize each text in the batch\n",
    "# tokenized_texts = []\n",
    "pixel_values = vl_chat_processor.image_processor(img, return_tensors=\"pt\").pixel_values\n",
    "img_embeddings = janus.aligner(janus.vision_model(pixel_values))\n",
    "print(img_embeddings.shape)\n",
    "\n",
    "joint_embeddings = []\n",
    "for i, single_text in enumerate(text):\n",
    "    single_text += vl_chat_processor.image_start_tag\n",
    "    input_ids = vl_chat_processor.tokenizer(single_text, return_tensors=\"pt\").input_ids[0]\n",
    "    print(vl_chat_processor.tokenizer.decode(input_ids, skip_special_tokens=False))\n",
    "    if input_ids.shape[0] > 150:\n",
    "        break\n",
    "    text_embedding = janus.language_model.get_input_embeddings()(input_ids).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    img_embedding = img_embeddings[i].unsqueeze(0)\n",
    "    joint_embedding = torch.cat((text_embedding, img_embedding), dim=1)\n",
    "    num_padding = 150 + 576 - joint_embedding.shape[1]\n",
    "    # print(joint_embedding.shape, num_padding)\n",
    "    joint_embedding = torch.cat((joint_embedding, torch.zeros(1, num_padding, 2048)), dim=1)\n",
    "\n",
    "    padding_mask = torch.ones(1, joint_embedding.shape[1], dtype=torch.bool)\n",
    "    padding_mask[:, -num_padding:] = 0\n",
    "    joint_embeddings.append(joint_embedding)\n",
    "    print(joint_embedding.shape)\n",
    "    # print(padding_mask)\n",
    "\n",
    "    print(joint_embedding[padding_mask].shape)\n",
    "\n",
    "joint_embeddings = torch.cat(joint_embeddings, dim=0)\n",
    "# tokenized_texts = torch.stack(tokenized_texts)\n",
    "# Display the first tokenized text\n",
    "# print(\"First tokenized text:\")\n",
    "# print(f\"Input IDs shape: {tokenized_texts[0].input_ids.shape}\")\n",
    "# print(f\"Input IDs: {tokenized_texts[:10]}... (truncated)\")\n",
    "# print(f\"Attention mask: {tokenized_texts[0].attention_mask[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c081418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 726, 2048])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32429c21",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mjanus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "inputs_embeds = janus.language_model.get_input_embeddings()(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddea16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42941749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "vision_model = janus.vision_model.to(\"cuda:1\")\n",
    "img_embed = vision_model(pixel_values.to(\"cuda:1\"))\n",
    "print(img_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc1fdcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image showcases the top of a grand building, featuring a large, glass-covered dome with a ribbed structure. Atop the dome sits an ornate golden statue of a winged figure, possibly an angel or a mythological being, holding a staff or scepter. The statue is perched on a golden sphere, adding to the opulent appearance. The architecture suggests a classical style, with intricate details and a sense of historical significance. The sky in the background is overcast, giving the scene a somewhat muted yet majestic atmosphere. The overall composition highlights the architectural beauty and the symbolic importance of the golden statue.',\n",
       " \"The image depicts an airport scene with a red and white airplane taxiing on a runway. The aircraft is positioned centrally in the frame, moving towards the camera. In the background, there is a control tower and a series of buildings, likely part of the airport infrastructure. The surrounding area includes greenery and trees, indicating a well-maintained environment. The sky appears clear with a few scattered clouds, suggesting good weather conditions. The runway markings are clearly visible, guiding the plane's movement. The overall setting conveys a sense of calm and order typical of an airport during non-peak hours.\",\n",
       " 'The image captures a serene nighttime scene with two glowing red lanterns floating in the sky, illuminated from within. The larger lantern is prominently positioned in the foreground, casting a warm, radiant light that contrasts sharply against the dark, starless night sky. In the background, a smaller lantern can be seen ascending higher into the darkness. Below, the silhouettes of buildings and trees are faintly visible, adding depth to the composition. The overall atmosphere is peaceful and contemplative, evoking a sense of tranquility and wonder often associated with lantern festivals or similar cultural celebrations.',\n",
       " 'The image depicts a blue New Holland tractor pulling a red trailer on a paved road in a rural setting. The tractor is equipped with large black tires and a yellow hubcap, and the trailer has a sturdy design with visible safety features such as a rear guard rail. The background shows a clear blue sky, green trees, and a distant landscape, suggesting a peaceful countryside environment. The road appears to be part of a farm or agricultural area, given the context of the machinery. The scene is well-lit, indicating it was taken during the daytime under sunny conditions.',\n",
       " 'The image depicts a charming urban street scene featuring a multi-story brick building with a Victorian architectural style. The building has a prominent central tower with a slate roof and arched windows, giving it a classic and historic appearance. The ground floor is lined with storefronts, each adorned with awnings, suggesting a mix of businesses or shops. Pedestrians can be seen walking along the sidewalk, adding a lively atmosphere to the scene. A white van is parked on the street, and other vehicles are visible in the background. The sky is clear and blue, indicating a pleasant day. The overall ambiance suggests a quaint, bustling neighborhood with a blend of historical charm and modern activity.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ec35550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store prompts for each item in the batch\n",
    "batch_prompts = []\n",
    "\n",
    "# Process each text in the batch\n",
    "for single_text in text:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"<|User|>\",\n",
    "            \"content\": single_text,  # Use the text from the batch\n",
    "        },\n",
    "        {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "    ]\n",
    "    \n",
    "    # Format the conversation\n",
    "    sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "        conversations=conversation,\n",
    "        sft_format=vl_chat_processor.sft_format,\n",
    "        system_prompt=\"\",\n",
    "    )\n",
    "    \n",
    "    # Add image start tag\n",
    "    prompt = sft_format + vl_chat_processor.image_start_tag\n",
    "    batch_prompts.append(prompt)\n",
    "\n",
    "# Now batch_prompts contains formatted prompts for each text in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0a32661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|User|>: The image showcases the top of a grand building, featuring a large, glass-covered dome with a ribbed structure. Atop the dome sits an ornate golden statue of a winged figure, possibly an angel or a mythological being, holding a staff or scepter. The statue is perched on a golden sphere, adding to the opulent appearance. The architecture suggests a classical style, with intricate details and a sense of historical significance. The sky in the background is overcast, giving the scene a somewhat muted yet majestic atmosphere. The overall composition highlights the architectural beauty and the symbolic importance of the golden statue.\\n\\n<|Assistant|>:<begin_of_image>',\n",
       " \"<|User|>: The image depicts an airport scene with a red and white airplane taxiing on a runway. The aircraft is positioned centrally in the frame, moving towards the camera. In the background, there is a control tower and a series of buildings, likely part of the airport infrastructure. The surrounding area includes greenery and trees, indicating a well-maintained environment. The sky appears clear with a few scattered clouds, suggesting good weather conditions. The runway markings are clearly visible, guiding the plane's movement. The overall setting conveys a sense of calm and order typical of an airport during non-peak hours.\\n\\n<|Assistant|>:<begin_of_image>\",\n",
       " '<|User|>: The image captures a serene nighttime scene with two glowing red lanterns floating in the sky, illuminated from within. The larger lantern is prominently positioned in the foreground, casting a warm, radiant light that contrasts sharply against the dark, starless night sky. In the background, a smaller lantern can be seen ascending higher into the darkness. Below, the silhouettes of buildings and trees are faintly visible, adding depth to the composition. The overall atmosphere is peaceful and contemplative, evoking a sense of tranquility and wonder often associated with lantern festivals or similar cultural celebrations.\\n\\n<|Assistant|>:<begin_of_image>',\n",
       " '<|User|>: The image depicts a blue New Holland tractor pulling a red trailer on a paved road in a rural setting. The tractor is equipped with large black tires and a yellow hubcap, and the trailer has a sturdy design with visible safety features such as a rear guard rail. The background shows a clear blue sky, green trees, and a distant landscape, suggesting a peaceful countryside environment. The road appears to be part of a farm or agricultural area, given the context of the machinery. The scene is well-lit, indicating it was taken during the daytime under sunny conditions.\\n\\n<|Assistant|>:<begin_of_image>',\n",
       " '<|User|>: The image depicts a charming urban street scene featuring a multi-story brick building with a Victorian architectural style. The building has a prominent central tower with a slate roof and arched windows, giving it a classic and historic appearance. The ground floor is lined with storefronts, each adorned with awnings, suggesting a mix of businesses or shops. Pedestrians can be seen walking along the sidewalk, adding a lively atmosphere to the scene. A white van is parked on the street, and other vehicles are visible in the background. The sky is clear and blue, indicating a pleasant day. The overall ambiance suggests a quaint, bustling neighborhood with a blend of historical charm and modern activity.\\n\\n<|Assistant|>:<begin_of_image>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae8248a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in batch_prompts:\n",
    "    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "    break\n",
    "# input_ids = torch.LongTensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcad1cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d1f4403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1597/1597 [00:00<00:00, 210446.56files/s]\n",
      "Generating train split: 28691 examples [00:07, 4041.49 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_BLIP3o \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data1/LargeData/BLIP3o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/load.py:2084\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2084\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2095\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/builder.py:925\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1649\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/builder.py:1001\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1008\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/builder.py:1487\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1485\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1487\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1488\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1489\u001b[0m     ):\n\u001b[1;32m   1490\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1491\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/builder.py:1608\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1607\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1608\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, record \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n\u001b[1;32m   1610\u001b[0m             num_examples, num_bytes \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/packaged_modules/webdataset/webdataset.py:122\u001b[0m, in \u001b[0;36mWebDataset._generate_examples\u001b[0;34m(self, tar_paths, tar_iterators)\u001b[0m\n\u001b[1;32m    120\u001b[0m all_field_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tar_idx, (tar_path, tar_iterator) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(tar_paths, tar_iterators)):\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example_idx, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pipeline_from_tar(tar_path, tar_iterator)):\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m field_name \u001b[38;5;129;01min\u001b[39;00m all_field_names:\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m field_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m example:\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/packaged_modules/webdataset/webdataset.py:32\u001b[0m, in \u001b[0;36mWebDataset._get_pipeline_from_tar\u001b[0;34m(cls, tar_path, tar_iterator)\u001b[0m\n\u001b[1;32m     30\u001b[0m fs: fsspec\u001b[38;5;241m.\u001b[39mAbstractFileSystem \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mfilesystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m streaming_download_manager \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mStreamingDownloadManager()\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename, f \u001b[38;5;129;01min\u001b[39;00m tar_iterator:\n\u001b[1;32m     33\u001b[0m     example_key, field_name \u001b[38;5;241m=\u001b[39m base_plus_ext(filename)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/utils/track.py:49\u001b[0m, in \u001b[0;36mTrackedIterableFromGenerator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs):\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_item \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/utils/file_utils.py:1343\u001b[0m, in \u001b[0;36mArchiveIterable._iter_from_urlpath\u001b[0;34m(cls, urlpath, download_config)\u001b[0m\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_zip(f)\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1343\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_tar(f)\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/datasets/utils/file_utils.py:1295\u001b[0m, in \u001b[0;36mArchiveIterable._iter_tar\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_iter_tar\u001b[39m(f):\n\u001b[1;32m   1294\u001b[0m     stream \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mopen(fileobj\u001b[38;5;241m=\u001b[39mf, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr|*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tarinfo \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m   1296\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m tarinfo\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/tarfile.py:2773\u001b[0m, in \u001b[0;36mTarFile.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2771\u001b[0m     tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmembers[index]\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded:\n\u001b[0;32m-> 2773\u001b[0m     tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2774\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tarinfo:\n\u001b[1;32m   2775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/tarfile.py:2631\u001b[0m, in \u001b[0;36mTarFile.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   2630\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2631\u001b[0m         tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromtarfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2632\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EOFHeaderError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2633\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_zeros:\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/tarfile.py:1296\u001b[0m, in \u001b[0;36mTarInfo.fromtarfile\u001b[0;34m(cls, tarfile)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the next TarInfo object from TarFile object\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;124;03m   tarfile.\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m buf \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mread(BLOCKSIZE)\n\u001b[0;32m-> 1296\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrombuf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m obj\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m BLOCKSIZE\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_proc_member(tarfile)\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/tarfile.py:1239\u001b[0m, in \u001b[0;36mTarInfo.frombuf\u001b[0;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EOFHeaderError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend of file header\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1238\u001b[0m chksum \u001b[38;5;241m=\u001b[39m nti(buf[\u001b[38;5;241m148\u001b[39m:\u001b[38;5;241m156\u001b[39m])\n\u001b[0;32m-> 1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chksum \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcalc_chksums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeaderError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad checksum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1242\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/tarfile.py:236\u001b[0m, in \u001b[0;36mcalc_chksums\u001b[0;34m(buf)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the checksum for a member's header by summing up all\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m   characters except for the chksum field which is treated as if\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m   it was filled with spaces. According to the GNU tar sources,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m   signed.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m unsigned_chksum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(struct\u001b[38;5;241m.\u001b[39munpack_from(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m148B8x356B\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf))\n\u001b[0;32m--> 236\u001b[0m signed_chksum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstruct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m148b8x356b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unsigned_chksum, signed_chksum\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds_BLIP3o = load_dataset(\"/data1/LargeData/BLIP3o\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59eb14be",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 171\u001b[0m     {\n\u001b[1;32m    172\u001b[0m         key: collate(\n\u001b[1;32m    173\u001b[0m             [d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map\n\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m     }\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m     {\n\u001b[0;32m--> 172\u001b[0m         key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m     }\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clone\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dl \u001b[38;5;241m=\u001b[39m DataLoader(ds_BLIP3o, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:43\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_iter)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:191\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    181\u001b[0m                 {\n\u001b[1;32m    182\u001b[0m                     key: collate(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 }\n\u001b[1;32m    187\u001b[0m             )\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `copy()` / `update(mapping)`\u001b[39;00m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# or `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    192\u001b[0m             key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    194\u001b[0m         }\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    198\u001b[0m             collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:192\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    181\u001b[0m                 {\n\u001b[1;32m    182\u001b[0m                     key: collate(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 }\n\u001b[1;32m    187\u001b[0m             )\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `copy()` / `update(mapping)`\u001b[39;00m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# or `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 192\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    194\u001b[0m         }\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    198\u001b[0m             collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dllm/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.JpegImagePlugin.JpegImageFile'>"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds_BLIP3o, batch_size=2, num_workers=0)\n",
    "\n",
    "for batch in dl:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dec82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janus.models.diff_mlp import SimpleMLPAdaLN\n",
    "\n",
    "diff_head = SimpleMLPAdaLN(\n",
    "            in_channels = 1024,\n",
    "            model_channels = 1024,\n",
    "            out_channels = 1024,\n",
    "            z_channels = 2048,\n",
    "            num_res_blocks = 2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6ec5840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"/data1/jjc/experiment/bi_tok/0525_janus_gen_mse/diff_head-bi_tok-0k\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dff0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of parameters in ckpt\n",
    "num_params = sum(p.numel() for p in diff_head.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44212a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = \"sa_{:06d}.tar\"\n",
    "files_to_process = [\"BLIP3o-Pretrain-Long-Caption/\" + file_pattern.format(i) for i in range(0, 100 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b118562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLIP3o-Pretrain-Long-Caption/sa_000000.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000001.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000002.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000003.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000004.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000005.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000006.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000007.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000008.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000009.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000010.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000011.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000012.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000013.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000014.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000015.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000016.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000017.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000018.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000019.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000020.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000021.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000022.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000023.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000024.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000025.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000026.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000027.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000028.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000029.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000030.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000031.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000032.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000033.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000034.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000035.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000036.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000037.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000038.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000039.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000040.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000041.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000042.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000043.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000044.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000045.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000046.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000047.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000048.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000049.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000050.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000051.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000052.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000053.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000054.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000055.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000056.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000057.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000058.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000059.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000060.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000061.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000062.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000063.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000064.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000065.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000066.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000067.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000068.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000069.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000070.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000071.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000072.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000073.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000074.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000075.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000076.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000077.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000078.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000079.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000080.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000081.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000082.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000083.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000084.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000085.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000086.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000087.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000088.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000089.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000090.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000091.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000092.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000093.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000094.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000095.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000096.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000097.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000098.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000099.tar',\n",
       " 'BLIP3o-Pretrain-Long-Caption/sa_000100.tar']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
